{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training image-based models as baseline\n",
    "\n",
    "\n",
    "General instructions: \n",
    "\n",
    "Fork this notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin model + Mixup alpha 0.4 + EWA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# KAGGLE USE THIS\n",
    "##################\n",
    "# %%bash\n",
    "# # for kaggle only \n",
    "# pip install attrdict\n",
    "# pip install timm\n",
    "# pip install pytorch-lightning==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from attrdict import AttrDict\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import copy\n",
    "import pickle\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "# additional lightning \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.callbacks.progress import ProgressBarBase\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import LightningDataModule, LightningModule\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# VM USE THIS\n",
    "##################\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from pawnet.utility import *\n",
    "\n",
    "\n",
    "##################\n",
    "# KAGGLE USE THIS\n",
    "##################\n",
    "# you need to import the utility script by uploading to kaggle as pawnet_utility/pawnet_utility.py\n",
    "# from pawnet_utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # check the package version to get reproducible env \n",
    "# # source: https://www.kaggle.com/rtatman/get-the-versions-of-imported-packages\n",
    "\n",
    "##################\n",
    "# KAGGLE USE THIS\n",
    "##################\n",
    "\n",
    "# import pkg_resources\n",
    "# import types\n",
    "# def get_imports():\n",
    "#     for name, val in globals().items():\n",
    "#         if isinstance(val, types.ModuleType):\n",
    "#             # Split ensures you get root package, \n",
    "#             # not just imported function\n",
    "#             name = val.__name__.split(\".\")[0]\n",
    "\n",
    "#         elif isinstance(val, type):\n",
    "#             name = val.__module__.split(\".\")[0]\n",
    "\n",
    "#         # Some packages are weird and have different\n",
    "#         # imported names vs. system names\n",
    "#         if name == \"PIL\":\n",
    "#             name = \"Pillow\"\n",
    "#         elif name == \"sklearn\":\n",
    "#             name = \"scikit-learn\"\n",
    "\n",
    "#         yield name\n",
    "# imports = list(set(get_imports()))\n",
    "\n",
    "# requirements = []\n",
    "# for m in pkg_resources.working_set:\n",
    "#     if m.project_name in imports and m.project_name!=\"pip\":\n",
    "#         requirements.append((m.project_name, m.version))\n",
    "\n",
    "# for r in requirements:\n",
    "#     print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load config\n",
    "# this object manages all the configurations\n",
    "##################\n",
    "# KAGGLE USE THIS\n",
    "##################\n",
    "# config_path = \"../input/config/config.yaml\"\n",
    "\n",
    "##################\n",
    "# VM USE THIS\n",
    "##################\n",
    "config_path = \"../config/config.yaml\"\n",
    "\n",
    "\n",
    "base_config_manager = BaseConfigLoader(config_path)\n",
    "\n",
    "# TODO: edit this for each base image model\n",
    "model_config = base_config_manager.load_config().model.swin_tiny4_w7_224_m04_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttrDict({'model_name': 'swin_tiny4_w7_224_mixup04_ema', 'pretrained': 'swin_tiny_patch4_window7_224', 'n_splits': 5, 'epoch': 20, 'batch_size': 64, 'num_workers': 4, 'dropout': 0.2, 'learning_rate': 1e-05, 'feature_map_out_size': 768, 'mixup': {'alpha': 0.4}, 'average': {'type': 'ema', 'start': 1}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "We will load the data by creating torch datasets as well as dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# KAGGLE USE THIS\n",
    "##################\n",
    "# this is specific to kaggle\n",
    "# if running in GCS, replace with our GCP bucket\n",
    "# get cache location of the dataset\n",
    "# GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "# file_path = base_config_manager.load_config().filepath.kaggle #\"/kaggle/input/petfinder-pawpularity-score/\"\n",
    "\n",
    "\n",
    "##################\n",
    "# VM USE THIS\n",
    "##################\n",
    "file_path = base_config_manager.load_config().filepath.gcs\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(file_path,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(file_path,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \n",
       "0          0      1        0      0          0     0     0           63  \n",
       "1          0      0        0      0          0     0     0           42  \n",
       "2          0      0        0      1          1     0     0           28  \n",
       "3          0      0        0      0          0     0     0           15  \n",
       "4          0      1        0      0          0     0     0           72  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4128bae22183829d2b5fea10effdb0c3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43a2262d7738e3d420d453815151079e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e429cead1848a298432a0acad014c9d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8f49844c382931444e68dffbe20228f4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  4128bae22183829d2b5fea10effdb0c3              1     0     1     0       0   \n",
       "1  43a2262d7738e3d420d453815151079e              0     1     0     0       0   \n",
       "2  4e429cead1848a298432a0acad014c9d              0     0     0     1       0   \n",
       "3  80bc3ccafcc51b66303c2c263aa38486              1     0     1     0       0   \n",
       "4  8f49844c382931444e68dffbe20228f4              1     1     1     0       1   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  \n",
       "0          1      1        0      0          1     0     1  \n",
       "1          0      1        1      0          0     0     0  \n",
       "2          1      1        1      0          1     1     1  \n",
       "3          0      0        0      0          0     1     0  \n",
       "4          1      0        1      0          1     1     0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train basic models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for training - lightning variant \n",
    "\n",
    "* create relevant transformations, validation splits\n",
    "* what this version differs is that we will be using the pytorch lightning framework - this allows easy TPU access for training \n",
    "\n",
    "lightning walkthrough: \n",
    "https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html\n",
    "<br>\n",
    "https://devblog.pytorchlightning.ai/train-anything-with-lightning-custom-loops-4be32314c961\n",
    "<br>\n",
    "https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/loop_examples/mnist_lite.py\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All pre-trained models expect input images normalized in the same way, \n",
    "i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), \n",
    "where H and W are expected to be at least 224. \n",
    "The images have to be loaded in to a range of [0, 1] and then \n",
    "normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. \n",
    "You can use the following transform to normalize:\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html\n",
    "\n",
    "\"\"\"\n",
    "train_transformation = T.Compose(\n",
    "            [\n",
    "                T.Resize([224,224]),# imgnet needs at least 224\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomVerticalFlip(),\n",
    "                T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ), # imgnet requirements \n",
    "            ]\n",
    "        )\n",
    "# train_data = pawnetDataset(annotation_df=train_df,img_dir = os.path.join(file_path,\"train\"),transform = train_transformation)\n",
    "# # batchsize should be parameter in config\n",
    "# train_loader = torch.utils.data.DataLoader(train_data,batch_size=64,num_workers =2, shuffle=True)\n",
    "\n",
    "\n",
    "test_transformation = T.Compose([\n",
    "                T.Resize([224,224]),# imgnet needs at least 224\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ), # imgnet requirements \n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load weights and get validation score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test_ema(model,valid_loader,criterion,device= \"cpu\"):\n",
    "    \"\"\"\n",
    "    variant of inference - uses wa_model for inference\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_valid = []\n",
    "    y_pred_valid = []\n",
    "    for i, (x,y) in enumerate(valid_loader):\n",
    "        with torch.no_grad():\n",
    "            pred = model.wa_model(x.to(device))\n",
    "            pred = torch.sigmoid(pred) * 100.\n",
    "            y_pred_valid.append(pred.squeeze().detach().cpu())\n",
    "            y_valid.append(y.detach().cpu())\n",
    "    # convert from list to tensor\n",
    "    y_valid = torch.cat(y_valid,0)\n",
    "    y_pred_valid = torch.cat(y_pred_valid,0)\n",
    "    if criterion is None:\n",
    "        valid_loss = None\n",
    "    else:\n",
    "        \n",
    "        valid_loss = criterion(y_pred_valid,y_valid).item()\n",
    "    \n",
    "    return valid_loss,y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting: fold 1\n",
      "will perform mixup\n",
      "\n",
      " Starting: fold 2\n",
      "will perform mixup\n",
      "\n",
      " Starting: fold 3\n",
      "will perform mixup\n",
      "\n",
      " Starting: fold 4\n",
      "will perform mixup\n",
      "\n",
      " Starting: fold 5\n",
      "will perform mixup\n"
     ]
    }
   ],
   "source": [
    "seed_everything(1)\n",
    "skf = StratifiedKFold(n_splits = model_config.n_splits, shuffle = True, random_state = 1)\n",
    "splits = skf.split(train_df[\"Id\"],train_df[\"Pawpularity\"])\n",
    "\n",
    "rmse_list = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "    print(\"\\n Starting: fold {}\".format(i+1))\n",
    "    X_train, X_valid = train_df.iloc[train_index], train_df.iloc[test_index]\n",
    "    X_train.reset_index(inplace=True,drop=True)\n",
    "    X_valid.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    valid_data = pawnetDataset(annotation_df=X_valid,img_dir =os.path.join(file_path,\"train\"), transform = test_transformation,test=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=64,shuffle=False,num_workers=2)\n",
    "    \n",
    "    model = pawNetAdvance.load_from_checkpoint(checkpoint_path=f\"swin_tiny4_w7_224_mixup04_ema/default/version_{i}/checkpoints/best_loss.ckpt\",criterion=criterion,model_config=model_config)\n",
    "    model = model.to(\"cuda\")\n",
    "    _,pred = inference_test(model,valid_loader,criterion=None,device=\"cuda\")\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(X_valid[\"Pawpularity\"],pred.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18.23613484255449,\n",
       " 18.388431573284382,\n",
       " 18.11324924461515,\n",
       " 18.14519259819677,\n",
       " 17.74394553323199]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.125390758376557"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rmse_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tensorboard (doesnt seem to work on kaggle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir ./pawnet_lightning_swin_tiny4_w7_224/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
