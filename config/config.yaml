filepath:
  kaggle: /kaggle/input/petfinder-pawpularity-score/
  gcs: ../data/

model:
  resnet34:
    model_name: pawnet_lightning_resnet # for tensorboard
    pretrained: resnet34
    n_splits: 5 # skf 
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 512 # the last feature map dim size
    mixup: null # disable mixup if null
    cutmix: null
  densenet121:
    model_name: pawnet_lightning_densenet # for tensorboard
    pretrained: densenet121
    n_splits: 5 # skf 
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 1024 # the last feature map dim size
    mixup: null
    cutmix: null
  vit_tiny16_224:
    model_name: pawnet_lightning_vit_tiny_patch16_224 # for tensorboard
    pretrained: vit_tiny_patch16_224
    n_splits: 5 # skf 
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 192 # the last feature map dim size
    mixup: null
    cutmix: null
  efficientnet_b0:
    model_name: pawnet_lightning_efficientnet_b0 # for tensorboard
    pretrained: efficientnet_b0
    n_splits: 5 # skf 
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 1280 # the last feature map dim size
    mixup: null
    cutmix: null
  swin_tiny4_w7_224:
    model_name: pawnet_lightning_swin_tiny4_w7_224 # for tensorboard
    pretrained: swin_tiny_patch4_window7_224
    n_splits: 5 # skf
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 768 # the last feature map dim size
    mixup: null
    cutmix: null
  # mix up experiments ==========================================================
  swin_tiny4_w7_224_m02:
    model_name: pawnet_lightning_swin_tiny4_w7_224_mixup02 # for tensorboard
    pretrained: swin_tiny_patch4_window7_224
    n_splits: 5 # skf
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 768 # the last feature map dim size
    mixup:
      alpha: 0.2
  swin_tiny4_w7_224_m04:
    model_name: pawnet_lightning_swin_tiny4_w7_224_mixup04 # for tensorboard
    pretrained: swin_tiny_patch4_window7_224
    n_splits: 5 # skf
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 768 # the last feature map dim size
    mixup:
      alpha: 0.4
  # cut mix experiments ============================================================
  swin_tiny4_w7_224_cut1:
    model_name: pawnet_lightning_swin_tiny4_w7_224_cutmix1 # for tensorboard
    pretrained: swin_tiny_patch4_window7_224
    n_splits: 5 # skf
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 768 # the last feature map dim size
    mixup: null
    cutmix:
      num_mix: 2
      prob: 0.5
      beta: 1.0
  # AVG models ==========================================================
  swin_tiny4_w7_224_m04_swa:
    model_name: swin_tiny4_w7_224_mixup04_swa # for tensorboard
    pretrained: swin_tiny_patch4_window7_224
    n_splits: 5 # skf
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 768 # the last feature map dim size
    mixup:
      alpha: 0.4
    average:
      type: "swa"
      start: 10 # 75% of epochs?
  swin_tiny4_w7_224_m04_ema:
    model_name: swin_tiny4_w7_224_mixup04_ema # for tensorboard
    pretrained: swin_tiny_patch4_window7_224
    n_splits: 5 # skf
    epoch: 20
    batch_size: 64
    num_workers: 4
    # params
    dropout: 0.2
    learning_rate: 0.00001
    feature_map_out_size: 768 # the last feature map dim size
    mixup:
      alpha: 0.4
    average:
      type: "ema"
      start: 1
  # larger models ==========================================================
  swin_large4_w7_224_m04:
    model_name: pawnet_lightning_swin_large4_w7_224_mixup04 # for tensorboard
    pretrained: swin_large_patch4_window7_224
    n_splits: 5 # skf
    epoch: 20
    batch_size: 32
    num_workers: 4
    # params
    dropout: 0.5
    learning_rate: 0.00001
    feature_map_out_size: 1536 # the last feature map dim size
    mixup:
      alpha: 0.4
  vit_large16_224:
    model_name: pawnet_lightning_vit_large_patch16_224 # for tensorboard
    pretrained: vit_large_patch16_224
    n_splits: 5 # skf
    epoch: 20
    batch_size: 16
    num_workers: 4
    # params
    dropout: 0.5
    learning_rate: 0.00001
    feature_map_out_size: 1024 # the last feature map dim size
    mixup:
      alpha: 0.4
